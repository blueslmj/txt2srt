#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
GPUè¯Šæ–­å·¥å…· - æ·±åº¦æ£€æŸ¥GPUé…ç½®é—®é¢˜
"""

import sys


def diagnose_gpu():
    """æ·±åº¦è¯Šæ–­GPUé…ç½®"""
    print("=" * 70)
    print("ğŸ”¬ GPU æ·±åº¦è¯Šæ–­å·¥å…·")
    print("=" * 70)
    print()
    
    # 1. æ£€æŸ¥PyTorch
    print("ğŸ“¦ 1/7: æ£€æŸ¥ PyTorch å®‰è£…...")
    print("-" * 70)
    try:
        import torch
        print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯GPUç‰ˆæœ¬
        if "+cu" in torch.__version__:
            cuda_ver = torch.__version__.split("+cu")[1]
            print(f"âœ… GPUç‰ˆæœ¬ (CUDA {cuda_ver})")
        elif "+cpu" in torch.__version__:
            print("âŒ è¿™æ˜¯CPUç‰ˆæœ¬ï¼")
            print("   éœ€è¦é‡æ–°å®‰è£…GPUç‰ˆæœ¬")
            print("   å‚è€ƒ: GPU_SETUP.md")
            return False
        else:
            print("âš ï¸ æ— æ³•ç¡®å®šæ˜¯CPUè¿˜æ˜¯GPUç‰ˆæœ¬")
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…")
        return False
    
    print()
    
    # 2. æ£€æŸ¥CUDAå¯ç”¨æ€§
    print("ğŸ® 2/7: æ£€æŸ¥ CUDA å¯ç”¨æ€§...")
    print("-" * 70)
    cuda_available = torch.cuda.is_available()
    print(f"CUDA å¯ç”¨: {'âœ… æ˜¯' if cuda_available else 'âŒ å¦'}")
    
    if not cuda_available:
        print()
        print("âŒ CUDAä¸å¯ç”¨çš„å¯èƒ½åŸå› :")
        print("   1. æ²¡æœ‰NVIDIAç‹¬ç«‹æ˜¾å¡")
        print("   2. NVIDIAé©±åŠ¨æœªå®‰è£…æˆ–è¿‡æœŸ")
        print("   3. CUDAå·¥å…·åŒ…ç‰ˆæœ¬ä¸åŒ¹é…")
        print()
        print("ğŸ“‹ è¯Šæ–­æ­¥éª¤:")
        print("   1. æ‰“å¼€è®¾å¤‡ç®¡ç†å™¨ï¼ŒæŸ¥çœ‹æ˜¯å¦æœ‰NVIDIAæ˜¾å¡")
        print("   2. è¿è¡Œå‘½ä»¤: nvidia-smi")
        print("   3. å¦‚æœnvidia-smiæ— æ³•è¿è¡Œï¼Œéœ€è¦å®‰è£…/æ›´æ–°é©±åŠ¨")
        return False
    
    print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    print()
    
    # 3. æ£€æŸ¥GPUç¡¬ä»¶
    print("ğŸ’» 3/7: æ£€æŸ¥ GPU ç¡¬ä»¶ä¿¡æ¯...")
    print("-" * 70)
    
    device_count = torch.cuda.device_count()
    print(f"GPU æ•°é‡: {device_count}")
    
    for i in range(device_count):
        print(f"\nğŸ¯ GPU {i}:")
        props = torch.cuda.get_device_properties(i)
        
        print(f"   åç§°: {torch.cuda.get_device_name(i)}")
        print(f"   æ˜¾å­˜: {props.total_memory / (1024**3):.2f} GB")
        print(f"   è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        print(f"   å¤šå¤„ç†å™¨: {props.multi_processor_count}")
        
        # è¯„ä¼°GPUç­‰çº§
        if "RTX 40" in torch.cuda.get_device_name(i):
            print("   ç­‰çº§: â­â­â­â­â­ æ——èˆ°çº§ï¼ˆæœ€å¼ºï¼‰")
        elif "RTX 30" in torch.cuda.get_device_name(i):
            print("   ç­‰çº§: â­â­â­â­ é«˜ç«¯")
        elif "RTX 20" in torch.cuda.get_device_name(i) or "GTX 16" in torch.cuda.get_device_name(i):
            print("   ç­‰çº§: â­â­â­ ä¸­é«˜ç«¯")
        elif "GTX 10" in torch.cuda.get_device_name(i):
            print("   ç­‰çº§: â­â­ ä¸­ç«¯ï¼ˆè¾ƒè€ï¼‰")
        elif "MX" in torch.cuda.get_device_name(i) or "Intel" in torch.cuda.get_device_name(i):
            print("   ç­‰çº§: â­ å…¥é—¨çº§/é›†æˆæ˜¾å¡")
            print("   âš ï¸ æ€§èƒ½æœ‰é™ï¼ŒåŠ é€Ÿæ•ˆæœå¯èƒ½ä¸æ˜æ˜¾")
        
        # æ£€æŸ¥æ˜¾å­˜
        if props.total_memory / (1024**3) < 2:
            print("   âš ï¸ æ˜¾å­˜è¾ƒå°ï¼Œå¯èƒ½å½±å“å¤„ç†å¤§æ¨¡å‹")
        elif props.total_memory / (1024**3) < 4:
            print("   âœ… æ˜¾å­˜å……è¶³ï¼Œé€‚åˆå°æ¨¡å‹ï¼ˆtiny, baseï¼‰")
        else:
            print("   âœ… æ˜¾å­˜å……è¶³ï¼Œå¯è¿è¡Œå¤§æ¨¡å‹ï¼ˆmedium, largeï¼‰")
    
    print()
    
    # 4. æ£€æŸ¥NVIDIAé©±åŠ¨
    print("ğŸ”§ 4/7: æ£€æŸ¥ NVIDIA é©±åŠ¨...")
    print("-" * 70)
    
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi', '--query-gpu=driver_version', '--format=csv,noheader'],
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            driver_version = result.stdout.strip()
            print(f"âœ… é©±åŠ¨ç‰ˆæœ¬: {driver_version}")
            
            # ç®€å•çš„ç‰ˆæœ¬æ£€æŸ¥
            try:
                major_ver = int(driver_version.split('.')[0])
                if major_ver >= 520:
                    print("   âœ… é©±åŠ¨ç‰ˆæœ¬è¾ƒæ–°ï¼Œæ€§èƒ½è‰¯å¥½")
                elif major_ver >= 470:
                    print("   âœ… é©±åŠ¨ç‰ˆæœ¬å¯ç”¨")
                else:
                    print("   âš ï¸ é©±åŠ¨ç‰ˆæœ¬è¾ƒè€ï¼Œå»ºè®®æ›´æ–°")
            except:
                pass
        else:
            print("âš ï¸ æ— æ³•è·å–é©±åŠ¨ç‰ˆæœ¬")
    except FileNotFoundError:
        print("âŒ nvidia-smi æœªæ‰¾åˆ°")
        print("   NVIDIAé©±åŠ¨å¯èƒ½æœªæ­£ç¡®å®‰è£…")
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥é©±åŠ¨æ—¶å‡ºé”™: {e}")
    
    print()
    
    # 5. æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    print("ğŸ’¾ 5/7: æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µ...")
    print("-" * 70)
    
    for i in range(device_count):
        torch.cuda.set_device(i)
        allocated = torch.cuda.memory_allocated(i) / (1024**3)
        reserved = torch.cuda.memory_reserved(i) / (1024**3)
        total = torch.cuda.get_device_properties(i).total_memory / (1024**3)
        
        print(f"GPU {i}:")
        print(f"   å·²åˆ†é…: {allocated:.2f} GB")
        print(f"   å·²ä¿ç•™: {reserved:.2f} GB")
        print(f"   æ€»è®¡: {total:.2f} GB")
        print(f"   å¯ç”¨: {total - allocated:.2f} GB")
        
        if (total - allocated) < 1:
            print("   âš ï¸ æ˜¾å­˜ä¸è¶³ï¼Œå¯èƒ½å½±å“æ€§èƒ½")
        else:
            print("   âœ… æ˜¾å­˜å……è¶³")
    
    print()
    
    # 6. æµ‹è¯•åŸºæœ¬GPUæ“ä½œ
    print("ğŸ§ª 6/7: æµ‹è¯•åŸºæœ¬ GPU æ“ä½œ...")
    print("-" * 70)
    
    try:
        # æµ‹è¯•ç®€å•æ“ä½œ
        x = torch.randn(100, 100).cuda()
        y = torch.randn(100, 100).cuda()
        z = torch.matmul(x, y)
        torch.cuda.synchronize()
        print("âœ… åŸºæœ¬GPUè®¡ç®—æ­£å¸¸")
        
        # æµ‹è¯•æ•°æ®ä¼ è¾“é€Ÿåº¦
        import time
        
        # CPU -> GPU
        x_cpu = torch.randn(1000, 1000)
        start = time.time()
        for _ in range(100):
            x_gpu = x_cpu.cuda()
            torch.cuda.synchronize()
        transfer_time = time.time() - start
        print(f"âœ… æ•°æ®ä¼ è¾“é€Ÿåº¦: {transfer_time:.3f}ç§’ (100æ¬¡ä¼ è¾“)")
        
        if transfer_time > 1.0:
            print("   âš ï¸ æ•°æ®ä¼ è¾“è¾ƒæ…¢ï¼Œå¯èƒ½æ˜¯PCIeå¸¦å®½é™åˆ¶")
        
    except Exception as e:
        print(f"âŒ GPUæ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print()
    
    # 7. ç»¼åˆå»ºè®®
    print("ğŸ’¡ 7/7: ç»¼åˆå»ºè®®")
    print("-" * 70)
    
    # æ£€æŸ¥GPUå‹å·
    gpu_name = torch.cuda.get_device_name(0)
    props = torch.cuda.get_device_properties(0)
    
    print("\nğŸ¯ æ€§èƒ½è¯„ä¼°:")
    
    # è¯„åˆ†ç³»ç»Ÿ
    score = 0
    
    # æ˜¾å­˜è¯„åˆ†
    vram_gb = props.total_memory / (1024**3)
    if vram_gb >= 8:
        score += 30
        print("âœ… æ˜¾å­˜å……è¶³ (+30åˆ†)")
    elif vram_gb >= 4:
        score += 20
        print("âœ… æ˜¾å­˜å¤Ÿç”¨ (+20åˆ†)")
    else:
        score += 10
        print("âš ï¸ æ˜¾å­˜è¾ƒå°‘ (+10åˆ†)")
    
    # è®¡ç®—èƒ½åŠ›è¯„åˆ†
    if props.major >= 8:
        score += 40
        print("âœ… æœ€æ–°æ¶æ„ (+40åˆ†)")
    elif props.major >= 7:
        score += 30
        print("âœ… ç°ä»£æ¶æ„ (+30åˆ†)")
    elif props.major >= 6:
        score += 20
        print("âœ… å¯ç”¨æ¶æ„ (+20åˆ†)")
    else:
        score += 10
        print("âš ï¸ è¾ƒè€æ¶æ„ (+10åˆ†)")
    
    # å‹å·è¯„åˆ†
    if "RTX 40" in gpu_name or "RTX 30" in gpu_name:
        score += 30
        print("âœ… é«˜ç«¯GPU (+30åˆ†)")
    elif "RTX 20" in gpu_name or "GTX 16" in gpu_name:
        score += 20
        print("âœ… ä¸­é«˜ç«¯GPU (+20åˆ†)")
    elif "GTX 10" in gpu_name:
        score += 15
        print("âœ… ä¸­ç«¯GPU (+15åˆ†)")
    else:
        score += 5
        print("âš ï¸ å…¥é—¨çº§GPU (+5åˆ†)")
    
    print()
    print(f"ğŸ“Š æ€»åˆ†: {score}/100")
    print()
    
    if score >= 80:
        print("ğŸŒŸ ä¼˜ç§€é…ç½®ï¼")
        print("   é¢„æœŸåŠ é€Ÿæ¯”: 20-50x")
        print("   10åˆ†é’ŸéŸ³é¢‘å¤„ç†æ—¶é—´: 15-30ç§’")
        print("   æ¨èæ¨¡å‹: medium, large")
    elif score >= 60:
        print("âœ… è‰¯å¥½é…ç½®")
        print("   é¢„æœŸåŠ é€Ÿæ¯”: 10-20x")
        print("   10åˆ†é’ŸéŸ³é¢‘å¤„ç†æ—¶é—´: 30-60ç§’")
        print("   æ¨èæ¨¡å‹: base, small")
    elif score >= 40:
        print("âš ï¸ åŸºæœ¬å¯ç”¨")
        print("   é¢„æœŸåŠ é€Ÿæ¯”: 3-10x")
        print("   10åˆ†é’ŸéŸ³é¢‘å¤„ç†æ—¶é—´: 1-3åˆ†é’Ÿ")
        print("   æ¨èæ¨¡å‹: tiny, base")
    else:
        print("âŒ æ€§èƒ½æœ‰é™")
        print("   é¢„æœŸåŠ é€Ÿæ¯”: <3x")
        print("   10åˆ†é’ŸéŸ³é¢‘å¤„ç†æ—¶é—´: 3-8åˆ†é’Ÿ")
        print("   æ¨è: è€ƒè™‘ä½¿ç”¨CPUæˆ–å‡çº§GPU")
    
    print()
    print("=" * 70)
    print("è¯Šæ–­å®Œæˆï¼")
    print("=" * 70)
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    success = diagnose_gpu()
    
    if not success:
        print()
        print("ğŸ”— å‚è€ƒæ–‡æ¡£:")
        print("   GPUé…ç½®æŒ‡å—: GPU_SETUP.md")
        print("   é—®é¢˜æ’æŸ¥: https://pytorch.org/get-started/locally/")
    
    print()


if __name__ == "__main__":
    main()

